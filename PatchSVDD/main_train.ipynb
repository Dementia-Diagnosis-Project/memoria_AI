{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35354,"status":"ok","timestamp":1709451314364,"user":{"displayName":"김건","userId":"06871501696369056241"},"user_tz":-540},"id":"QlD0mYpPblEJ","outputId":"6a90fa0f-4620-4a09-ef9d-a163dbfa93ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6430,"status":"ok","timestamp":1709451320775,"user":{"displayName":"김건","userId":"06871501696369056241"},"user_tz":-540},"id":"pfktrq0kbU_L","outputId":"2700dce3-f310-482d-e853-276e6c858a42"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ngt\n","  Downloading ngt-2.1.6-cp310-cp310-manylinux_2_28_x86_64.whl (10.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ngt) (1.25.2)\n","Collecting pybind11 (from ngt)\n","  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pybind11, ngt\n","Successfully installed ngt-2.1.6 pybind11-2.11.1\n"]}],"source":["!pip install ngt"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1709451320777,"user":{"displayName":"김건","userId":"06871501696369056241"},"user_tz":-540},"id":"1NK7TlRKbw4X"},"outputs":[],"source":["import os\n","import sys\n","os.chdir('/content/drive/MyDrive/MRI_Anomaly/colab_practice')\n","# for path in sys.path:\n","#     print(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GT2xYRsubTwY","outputId":"fb05bb95-0e46-4fd8-a823-0b22790b0d84"},"outputs":[{"name":"stdout","output_type":"stream","text":["start!\n","Newwork Loading\n","Make Datasets\n","fpattern : /content/drive/MyDrive/MRI_Anomaly/train/*/*/*.png\n","fpattern 내 개수 : 5041\n"]}],"source":["import argparse\n","import torch\n","from codes import mvtecad\n","from functools import reduce\n","from torch.utils.data import DataLoader\n","from codes.datasets import *\n","from codes.networks import *\n","from codes.inspection import eval_encoder_NN_multiK\n","from codes.utils import *\n","\n","# SVDD Loss를 lambda_value만큼 반영 e.g. 0.8 or 1 ...\n","\n","def train():\n","    lambda_value = 1.0\n","    D = 64\n","    epochs=50\n","    lr=1e-4\n","    rep=100\n","    name = f'final_lambda{lambda_value}_repeat{rep}'\n","    print('start!')\n","    # D = args.D\n","    # lr = args.lr\n","    # rep = args.rep\n","    # name = args.name\n","\n","    # with task() 컨택스트 매니저 자원관리 효용성을 높임\n","    # utils.py에 있다.\n","    with task('Networks'):\n","        print('Newwork Loading')\n","        # 신경망들은 networks.py에서 관리됨\n","        # SVDD를 위한 인코더 신경망의 파라미터를 GPU에 load\n","        enc = EncoderHier(64, D).cuda()\n","        # 64*64 패치를 가지고 SSL을 위한 포지션 분류기 신경망의 파라미터 GPU에 load\n","        cls_64 = PositionClassifier(64, D).cuda()\n","        # 32*32 패치를 가지고 SSL을 위한 포지션 분류기 신경망의 파라미터 GPU에 load\n","        cls_32 = PositionClassifier(32, D).cuda()\n","\n","        # 위 모델들을 modules라는 리스트로 관리\n","        modules = [enc, cls_64, cls_32]\n","        # 세 개의 신경망의 학습할 파라미터를 params라는 리스트로 관리\n","        params = [list(module.parameters()) for module in modules]\n","        params = reduce(lambda x, y: x + y, params)\n","\n","        opt = torch.optim.Adam(params=params, lr=lr)\n","\n","    # 데이터 세트 만듦\n","    with task('Datasets'):\n","        print('Make Datasets')\n","        train_x = mvtecad.get_x_standardized(mode='train')\n","        train_x = NHWC2NCHW(train_x)\n","\n","        # rep = 100\n","        datasets = dict()\n","\n","        # 64*64 패치 생성 for SSL을 위한 데이터 세트\n","        datasets[f'pos_64'] = PositionDataset(train_x, K=64, repeat=rep)\n","        # 32*32 패치 생성 for SSL을 위한 데이터 세트\n","        datasets[f'pos_32'] = PositionDataset(train_x, K=32, repeat=rep)\n","        # 64*64 패치 생성 for SVDD를 위한 데이터 세트\n","        datasets[f'svdd_64'] = SVDD_Dataset(train_x, K=64, repeat=rep)\n","        # 32*32 패치 생성 for SVDD를 위한 데이터 세트\n","        datasets[f'svdd_32'] = SVDD_Dataset(train_x, K=32, repeat=rep)\n","\n","        # DictionaryConcatDataset는 utils.py에 존재\n","        # SSL과 SVDD 작업을 위한 데이터세트를 딕셔너리 형태로 관리?\n","\n","        dataset = DictionaryConcatDataset(datasets)\n","        loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n","\n","    best_det_mult = 0.0\n","    best_seg_mult = 0.0\n","    print('Start training')\n","    for i_epoch in range(epochs):\n","        print(f'Epoch : {i_epoch+1} 시작')\n","        if i_epoch != 0:\n","            # 3개의 신경망을 각각 학습 모드로 변경\n","            for module in modules:\n","                module.train()\n","\n","            # loader\n","            print('Loading networks!')\n","            for d in loader:\n","                d = to_device(d, 'cuda', non_blocking=True)\n","                opt.zero_grad()\n","\n","                # 64*64 SSl loss함수 계산\n","                # 학습된 cls_64,  EncoderHier(64, D) 신경망을 가지고, datasets[f'pos_64'] 데이터를 가지고 SSL의 loss 계산\n","\n","                loss_pos_64 = PositionClassifier.infer(cls_64, enc, d['pos_64'])\n","\n","                # 32*32 SSl loss함수 계산 enc.enc\n","\n","                loss_pos_32 = PositionClassifier.infer(cls_32, enc.enc, d['pos_32'])\n","\n","\n","                # 64*64 패치 loss함수 계산\n","                # EncoderHier(64, D)로 datasets[f'svdd_64'] 데이터를 가지고 SVDD의 loss 계산\n","\n","                loss_svdd_64 = SVDD_Dataset.infer(enc, d['svdd_64'])\n","\n","                # 32*32 패치 loss함수 계산\n","\n","                loss_svdd_32 = SVDD_Dataset.infer(enc.enc, d['svdd_32'])\n","\n","\n","                # Patch SVDD의 Totla loss\n","                loss = loss_pos_64 + loss_pos_32 + lambda_value * (loss_svdd_64 + loss_svdd_32)\n","\n","                loss.backward()\n","                opt.step()\n","\n","        aurocs = eval_encoder_NN_multiK(enc)\n","        print(f'Epoch : {i_epoch+1} 성능 평가')\n","        print(f'loss_pos_64 : {loss_pos_64.item():.4f}, loss_pos_32 : {loss_pos_32.item():.4f}, loss_svdd_64 : {loss_svdd_64.item():.4f}, loss_svdd_32 : {loss_svdd_32.item():.4f}')\n","        print(f'Total loss : {loss.item():.4f}\\n')\n","\n","        det_mult, seg_mult = log_result(name, aurocs)\n","        enc.save(name)\n","\n","        if det_mult > best_det_mult and seg_mult > best_seg_mult:\n","            best_det_mult = det_mult\n","            best_seg_mult = seg_mult\n","\n","            enc.save(f'{i_epoch+1}_{name}')\n","\n","        del aurocs\n","\n","\n","\n","\n","def log_result(name, aurocs):\n","    det_64 = aurocs['det_64'] * 100\n","    seg_64 = aurocs['seg_64'] * 100\n","\n","    det_32 = aurocs['det_32'] * 100\n","    seg_32 = aurocs['seg_32'] * 100\n","\n","    det_sum = aurocs['det_sum'] * 100\n","    seg_sum = aurocs['seg_sum'] * 100\n","\n","    det_mult = aurocs['det_mult'] * 100\n","    seg_mult = aurocs['seg_mult'] * 100\n","\n","    print(f'|K64| Det: {det_64:.4f} Seg: {seg_64:.4f} |K32| Det: {det_32:.4f} Seg: {seg_32:.4f} |sum| Det: {det_sum:.4f} Seg: {seg_sum:.4f} |mult| Det: {det_mult:.4f} Seg: {seg_mult:.4f} ({name})')\n","\n","    return det_mult, seg_mult\n","\n","if __name__ == '__main__':\n","    train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VrlktrxdbnS"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPwUJKABuhD7QcFhgiD6mjN","gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
